# Gestforms
In this ongoing project, we want to test if 18-months old infants display selective attention to gesture forms as compared to motor actions.
This question is embedded in the biger inquiry of how do infants detect linguistic signals in their environments early on in life, which facilitates later language acquisition. While most past research has focussed on infants' selective 
preferences for signals in the auditory modality (e.g. speech sounds), mechanisms driving early detection of linguistic signals in the visual modality (e.g. gestures) is rarely studied. 
Gestforms aims at addressing this gap in the literature to inform us further on the early development of multimodal language in infants.


## Extracting data:
Using a Python script (test.py),  we first converted raw EyeLink .asc eye-tracking files which processes and classifies gaze points into Regions of Interest (ROIs) during experimental trials involving gesture (target) and action (distractor) videos.
At this step, the main goal is to determine where participants were looking over time and whether their gaze was directed toward the target gesture, the distractor action, or outside these two regions.

The resulting .txt file is in this repository, available to dowload.
